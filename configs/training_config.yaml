## =============================================================================
## Training Configuration
## =============================================================================
##
## Two training modes are supported via the `training_mode` key:
##
##   scratch   - Train a timm ViT or ResNet from scratch on ImageNet using
##               CrossEntropyLoss.  Expects img_dir/train/ and img_dir/val/.
##               All perturbation types except `random_target` are supported.
##
##   finetune  - Fine-tune a CLIP-HBA model with DoRA adaptation on THINGS
##               (or ImageNet) targets using MSELoss.  All perturbation types
##               including `random_target` are supported.
##
## Set `training_mode` below and fill in the relevant section.
## =============================================================================


## ---TRAINING MODE--- ##
training_mode: scratch        # scratch | finetune


## ---PATHS--- ##
save_path: "/path/to/save/training/artifacts"

# From-scratch: root ImageNet directory (must contain train/ and val/ folders)
img_dir: "/path/to/imagenet"

# Fine-tuning only: annotation CSV and THINGS image directory
img_annotations_file:         # ./data/spose_embedding66d_rescaled_1806train.csv
baseline_checkpoint_path:     # required only when perturb_epoch > 0


## ---LOGGING & W&B--- ##
wandb_project:                # Optional: W&B project name (offline if blank)
wandb_entity:                 # Optional: W&B username / team

debug_logging: false          # Log per-batch debug info to console


## ---TRAINING PARAMETERS--- ##
dataset_type: imagenet        # imagenet | things
epochs: 90                    # Total training epochs (90 is standard ImageNet)
early_stopping_patience: 20   # Epochs without val improvement before stopping
batch_size: 256               # Samples per batch
train_portion: 0.8            # Finetune only: train/val split ratio
lr: 0.1                       # Learning rate (0.1 SGD-style or 3e-4 AdamW)
criterion: CrossEntropyLoss   # CrossEntropyLoss (scratch) | MSELoss (finetune)
random_seed: 1                # Global random seed
cuda: 0                       # -1 = all GPUs | 0 = GPU 0 | 1 = GPU 1


## ---PERTURBATION PARAMETERS--- ##
## Supported types: none | image_noise | uniform_images | label_shuffle
##   + random_target (finetune only â€” requires continuous embedding targets)
perturb_type: none            # Perturbation strategy
perturb_epoch:                # First epoch (0-indexed) to apply perturbation
perturb_length:               # Number of consecutive epochs to perturb
perturb_seed:                 # RNG seed for deterministic perturbations


## ---MODEL PARAMETERS (SCRATCH)--- ##
## Used when training_mode: scratch
architecture: ViT-B/16        # ViT-B/16 | ViT-L/14 | RN50 | RN101
pretrained: false             # Load timm pretrained weights (false = from scratch)
num_classes: 1000             # Output classes (1000 for standard ImageNet)


## ---MODEL PARAMETERS (FINETUNE / CLIP-HBA)--- ##
## Used when training_mode: finetune  (ignored for scratch)
# architecture: CLIP-HBA
# clip_hba_backbone: ViT-L/14     # CLIP backbone: ViT-B/16 | ViT-L/14 | RN50
# vision_layers: 2                 # Vision encoder DoRA layers
# transformer_layers: 1            # Text encoder DoRA layers
# rank: 32                         # DoRA low-rank dimension


## ---RSA PARAMETERS (FINETUNE ONLY)--- ##
behavioral_rsa: false         # Compute behavioral RSA each epoch (finetune only)
rsa_annotations_file:         # ./data/spose_embedding66d_rescaled_1806train.csv
model_rdm_distance_metric: pearson   # pearson | cosine
rsa_similarity_metric: spearman      # spearman | pearson
