
  backbone: ViT-L/14
  vision_layers: 2
  transformer_layers: 1
  rank: 32


  epochs: 500
  batch_size: 64
  train_portion: 0.8
  lr: 3e-4
  early_stopping_patience: 20
  criterion: MSELoss
  random_seed: 2
  cuda: 1 # -1 for all GPUs, 0 for GPU 0, 1 for GPU 1, 2 for CPU


  dataset_type: things
  img_annotations_file: clip_hba_behavior/Data/spose_embedding66d_rescaled_1806train.csv # csv data annotations of the training stimuli with the corresponding target embeddings
  img_dir: /home/wallacelab/teba/multimodal_brain_inspired/marren/cvpr_perturbations/clip_hba_behavior/Data/Things1854 # path to the image directory


  logger: None
  checkpoint_path: /home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test/cliphba_baseline_seed2 # path to save the trained model weights
  training_res_path: /home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test/cliphba_baseline_seed2/training_res.csv # location to save the training results
  random_state_path: /home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test/cliphba_baseline_seed2/random_states # location to save the random states at every epoch