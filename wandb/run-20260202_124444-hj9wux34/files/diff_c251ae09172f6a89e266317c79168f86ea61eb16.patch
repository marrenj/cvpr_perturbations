diff --git a/configs/training_config.yaml b/configs/training_config.yaml
index ab2144d..5ef8387 100644
--- a/configs/training_config.yaml
+++ b/configs/training_config.yaml
@@ -1,11 +1,15 @@
+## ---WANDB CONFIGURATION--- ##
+  'wandb_project': temporal-dynamics-of-human-alignment # Required
+  'wandb_entity': # Optional: your username or team name
+
 ## ---EXPERIMENT SETUP--- ##
   experiment_type: baseline # baseline | perturbation_run
 
 ## ---PATHS--- ##
-  save_path: # root path to save training artifacts (checkpoints, training results, random states)
-  baseline_checkpoint_path: # REQUIRED: path to baseline training run checkpoints used for perturbation sweeps
-  img_annotations_file: # path to the csv file containing the image names and the corresponding target embeddings for the THINGS training data
-  img_dir: # path to the THINGS image directory
+  save_path: "/home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test" # root path to save training artifacts (checkpoints, training results, random states)
+  baseline_checkpoint_path: # path to baseline training run checkpoints used for perturbation sweeps
+  img_annotations_file: "./data/spose_embedding66d_rescaled_1806train.csv" # path to the csv file containing the image names and the corresponding target embeddings for the THINGS training data
+  img_dir: "/home/wallacelab/investigating-complexity/Images/THINGS" # path to the THINGS image directory
 
 ## ---MODEL PARAMETERS--- ##
   backbone: ViT-L/14 # model backboneto use for CLIP-HBA
@@ -27,7 +31,7 @@
   logger: None # Logger to use for training. None for no logging.
 
 ## ---PERTURBATION PARAMETERS--- ##
-  perturb_type: label_shuffle
-  perturb_epoch: 0 # epoch to start perturbation (0-indexed)
-  perturb_length: 1 # number of epochs to perturb
-  perturb_seed: 42
\ No newline at end of file
+  perturb_type: none
+  perturb_epoch:  # epoch to start perturbation (0-indexed)
+  perturb_length: # number of epochs to perturb
+  perturb_seed:
\ No newline at end of file
diff --git a/scripts/run_training.py b/scripts/run_training.py
index d062971..efd6eb9 100755
--- a/scripts/run_training.py
+++ b/scripts/run_training.py
@@ -116,13 +116,15 @@ def _prepare_single_run(config: dict, config_path: Path) -> dict:
     if not base_save_path:
         raise ValueError("Config must set 'save_path'.")
 
-    perturb_type = config.get("perturb_type", "baseline")
-    perturb_seed = config.get("perturb_seed")
-    perturb_epoch = int(config.get("perturb_epoch"))
-    perturb_length = int(config.get("perturb_length"))
+    perturb_type = str(config.get("perturb_type") or "none")
     random_seed = config.get("random_seed")
+    perturb_seed = config.get("perturb_seed")
+    # Default to zeroed perturb settings for baseline (none) runs
+    perturb_epoch = int(config.get("perturb_epoch") or 0)
+    perturb_length = int(config.get("perturb_length") or 0)
+    perturb_seed = int(perturb_seed if perturb_seed is not None else (random_seed or 0))
 
-    if perturb_type and str(perturb_type).lower() != "none":
+    if perturb_type and perturb_type.lower() != "none":
         # Include epoch/length tokens for perturbation runs
         seed_suffix = f"perturb_seed{perturb_seed}"
         run_root = Path(base_save_path) / f"{perturb_type}_{seed_suffix}" / f"epoch{perturb_epoch}_length{perturb_length}"
diff --git a/src/training/__pycache__/trainer.cpython-311.pyc b/src/training/__pycache__/trainer.cpython-311.pyc
index e716df3..bc4fc37 100644
Binary files a/src/training/__pycache__/trainer.cpython-311.pyc and b/src/training/__pycache__/trainer.cpython-311.pyc differ
diff --git a/src/training/trainer.py b/src/training/trainer.py
index c40e08b..a3d07d1 100644
--- a/src/training/trainer.py
+++ b/src/training/trainer.py
@@ -9,6 +9,7 @@ and checkpoint resumption for reproducible experiments.
 import os
 import random
 import csv
+import wandb
 from tqdm import tqdm
 from numpy.random import set_state as np_set_state
 import torch 
@@ -41,6 +42,106 @@ from src.perturbations.perturbation_utils import choose_perturbation_strategy
 # Path Setup
 # =============================================================================
 
+
+def get_wandb_tags(config):
+    """
+    Generate tags for wandb run based on configuration.
+    
+    Args:
+        config: Configuration dictionary
+        
+    Returns:
+        list: List of tags
+    """
+    tags = [
+        config['backbone'],
+        f"rank_{config['rank']}",
+        config['perturb_type'],
+        config['dataset_type']
+    ]
+    
+    if config['perturb_type'] != 'none':
+        tags.append(f"perturb_epoch_{config['perturb_epoch']}")
+        tags.append(f"perturb_length_{config['perturb_length']}")
+        tags.append(f"perturb_seed_{config['perturb_seed']}")
+        tags.append(f"random_seed_{config['random_seed']}")
+    
+    if 'wandb_tags' in config and config['wandb_tags']:
+        tags.extend(config['wandb_tags'])
+    
+    return tags
+
+
+def get_run_name(config):
+    """
+    Build a deterministic run name shared by wandb and local save paths.
+    """
+    if config.get('wandb_run_name'):
+        return config['wandb_run_name']
+    return (
+        f"{config['backbone']}_r{config['rank']}_"
+        f"perturb_{config['perturb_type']}_"
+        f"epoch{config['perturb_epoch']}_"
+        f"length{config['perturb_length']}_"
+        f"perturb-seed{config['perturb_seed']}_"
+        f"init-seed{config['random_seed']}"
+    )
+
+
+def init_wandb(config, resume_epoch=0):
+    """
+    Initialize Weights & Biases run with proper configuration.
+    
+    Args:
+        config: Configuration dictionary
+        resume_epoch: Epoch to resume from
+        
+    Returns:
+        wandb.Run: Weights & Biases run
+    """
+    resume_mode = "allow" if resume_epoch > 0 else None
+    run_id = config.get('wandb_run_id', None)
+    
+    # Create descriptive run name (shared with filesystem path)
+    run_name = get_run_name(config)
+    
+    run = wandb.init(
+        project=config.get('wandb_project', 'clip-hba-training'),
+        entity=config.get('wandb_entity', None),
+        name=run_name,
+        id=run_id,
+        resume=resume_mode,
+        config=config,
+        tags=get_wandb_tags(config),
+        notes=config.get('wandb_notes', ''),
+        save_code=True,
+    )
+    
+    config['wandb_run_id'] = run.id
+    config['wandb_run_name'] = run.name or run_name
+    return run
+
+
+def log_model_architecture(model, logger):
+    """
+    Log model architecture details to wandb.
+    """
+    total_params = sum(p.numel() for p in model.parameters())
+    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
+    
+    wandb.config.update({
+        'total_parameters': total_params,
+        'trainable_parameters': trainable_params,
+        'trainable_percentage': 100 * trainable_params / total_params if total_params > 0 else 0
+    })
+    
+    trainable_layers = [name for name, param in model.named_parameters() if param.requires_grad]
+    wandb.config.update({'trainable_layers': trainable_layers})
+    
+    logger.info(f"Total parameters: {total_params:,}")
+    logger.info(f"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
+
+
 def setup_paths(config):
     """
     Create and return all necessary directory paths for training outputs.
@@ -71,7 +172,7 @@ def setup_paths(config):
                     raise FileExistsError(f"Aborted because {description} '{path}' already exists.")
                 else:
                     print("Please answer 'Yes' or 'No'.")
-                return config['save_path'], training_results_save_path, random_state_save_path, checkpoints_save_path
+    return config['save_path'], training_results_save_path, random_state_save_path
 
 
 # =============================================================================
@@ -133,6 +234,13 @@ def setup_dataset(config, logger):
             'random_seed': config['random_seed'],
             'train_portion': config['train_portion']
         }
+
+    wandb.config.update({
+        'dataset_size': len(dataset),
+        'train_size': len(train_dataset),
+        'test_size': len(test_dataset),
+        'train_portion': config['train_portion']
+    })
     
     return train_dataset, test_dataset, split_info, (global_target_mean, global_target_std)
 
@@ -210,6 +318,9 @@ def setup_model(config, device):
         model = DataParallel(model)
     
     model.to(device)
+
+    if config.get('wandb_watch_model', True):
+        wandb.watch(model, log='all', log_freq=config.get('wandb_log_freq', 100))
     
     return model
 
@@ -363,7 +474,7 @@ def handle_checkpoint_resumption(config, model, optimizer, dataloader_generator,
 
 
 def train_one_epoch(model, train_loader, device, optimizer, criterion, 
-                   perturb_strategy, epoch_idx, logger):
+                   perturb_strategy, epoch_idx, logger, log_interval=10):
     """
     Train model for a single epoch.
     
@@ -376,16 +487,21 @@ def train_one_epoch(model, train_loader, device, optimizer, criterion,
         perturb_strategy: Perturbation strategy to apply
         epoch_idx: Current epoch index
         logger: Logger instance
+        log_interval: Interval to log metrics with wandb
         
     Returns:
         float: Average training loss for the epoch
     """
     model.train()
     total_loss = 0.0
+    batch_losses = []
     
     # Log if perturbation is active this epoch
     if perturb_strategy.is_active_epoch(epoch_idx):
         logger.info(f"Applying {perturb_strategy.__class__.__name__} perturbation during epoch {epoch_idx}")
+        wandb.log({'perturbation_active': 1, 'epoch': epoch_idx})
+    else:
+        wandb.log({'perturbation_active': 0, 'epoch': epoch_idx})
     
     progress_bar = tqdm(
         enumerate(train_loader), 
@@ -413,10 +529,26 @@ def train_one_epoch(model, train_loader, device, optimizer, criterion,
         optimizer.step()
         
         # Update metrics
+        batch_loss = loss.item()
         total_loss += loss.item() * images.size(0)
-        progress_bar.set_postfix({'loss': loss.item()})
+        batch_losses.append(batch_loss)
+        progress_bar.set_postfix({'loss': batch_loss})
+
+        if batch_idx % log_interval == 0:
+            wandb.log({
+                'batch_loss': batch_loss,
+                'batch': epoch_idx * len(train_loader) + batch_idx,
+                'epoch': epoch_idx,
+            })
     
     avg_train_loss = total_loss / len(train_loader.dataset)
+
+    wandb.log({
+        'train_loss': avg_train_loss,
+        'train_loss_std': np.std(batch_losses),
+        'epoch': epoch_idx,
+    })
+
     return avg_train_loss
 
 
@@ -490,6 +622,11 @@ def train_model(
     best_test_loss = evaluate_model(model, test_loader, device, criterion)
     log(f"Initial Validation Loss: {best_test_loss:.4f}")
     log("*********************************\n")
+
+    wandb.log({
+        'initial_val_loss': best_test_loss,
+        'epoch': -1,
+    })
     
     # Create directories for outputs
     os.makedirs(save_path, exist_ok=True)
@@ -500,6 +637,7 @@ def train_model(
         with open(training_results_save_path, 'w', newline='') as file:
             writer = csv.writer(file)
             writer.writerow(['epoch', 'train_loss', 'test_loss'])
+
     # Main training loop
     for epoch in range(start_epoch, epochs):
         # Train one epoch
@@ -517,6 +655,12 @@ def train_model(
         
         if perturb_strategy.is_active_epoch(epoch):
             logger.info(f"*** Perturbation '{perturb_strategy.__class__.__name__}' was applied during epoch {epoch} ***")
+
+        wandb.log({
+            'val_loss': avg_test_loss,
+            'epoch': epoch,
+            'learning_rate': optimizer.param_groups[0]['lr'],
+        })
         
         # Save metrics to CSV
         save_epoch_results(epoch, avg_train_loss, avg_test_loss, training_results_save_path)
@@ -535,20 +679,48 @@ def train_model(
             log_fn=log,
         )
         log(f"Checkpoint saved for epoch {epoch}")
+
+        if epoch % 5 == 0:  # Save every 5 epochs to wandb
+            artifact = wandb.Artifact(
+                name=f"model-checkpoint-epoch-{epoch}",
+                type="model",
+                description=f"Model checkpoint at epoch {epoch}",
+                metadata={
+                    'epoch': epoch,
+                    'train_loss': avg_train_loss,
+                    'val_loss': avg_test_loss,
+                }
+            )
+            artifact.add_dir(dora_params_dir)
+            wandb.log_artifact(artifact)
         
         # Early stopping check
         if avg_test_loss < best_test_loss:
             best_test_loss = avg_test_loss
             epochs_no_improve = 0
+            wandb.run.summary["best_val_loss"] = best_test_loss
+            wandb.run.summary["best_epoch"] = epoch
         else:
             epochs_no_improve += 1
+
+        wandb.log({
+            'epochs_no_improve': epochs_no_improve,
+            'best_val_loss': best_test_loss,
+            'epoch': epoch,
+        })
         
         if epochs_no_improve == early_stopping_patience:
             log("\n\n*********************************")
             log(f"Early stopping triggered at epoch {epoch}")
             log("*********************************\n\n")
+            wandb.run.summary["stopped_early"] = True
+            wandb.run.summary["final_epoch"] = epoch
             break
 
+    if epochs_no_improve < early_stopping_patience:
+        wandb.run.summary["stopped_early"] = False
+        wandb.run.summary["final_epoch"] = epochs
+
 
 # =============================================================================
 # Main Training Orchestration
@@ -572,6 +744,22 @@ def run_training_experiment(config):
     # Set random seed for reproducibility
     seed_everything(config['random_seed'])
     
+    # Build run name once so wandb and filesystem stay in sync
+    run_name = get_run_name(config)
+    config['wandb_run_name'] = run_name
+    
+    # Align save_path with the run name (append unless already present)
+    base_save_path = config.get('save_path', '')
+    if base_save_path:
+        normalized_base = os.path.normpath(base_save_path)
+        if os.path.basename(normalized_base) != run_name:
+            save_path = os.path.join(base_save_path, run_name)
+        else:
+            save_path = base_save_path
+    else:
+        save_path = run_name
+    config['save_path'] = save_path
+    
     # Setup logging
     os.makedirs(config['save_path'], exist_ok=True)
     log_file = os.path.join(config['save_path'], f'training_log_{config["perturb_type"]}.txt')
@@ -583,7 +771,10 @@ def run_training_experiment(config):
     logger.info("="*80)
     
     # Setup paths
-    save_path, training_results_save_path, random_state_save_path = setup_paths(config['save_path'])
+    save_path, training_results_save_path, random_state_save_path = setup_paths(config)
+
+    # Initialize wandb before any wandb.config updates in dataset setup
+    wandb_run = init_wandb(config, resume_epoch=0)
     
     # Setup dataset
     train_dataset, test_dataset, split_info, target_stats = setup_dataset(config, logger)
@@ -601,7 +792,7 @@ def run_training_experiment(config):
     )
     
     # Setup device
-    device = get_device(config['cuda'])
+    device = get_device(config['cuda'], logger)
     
     # Setup model
     model = setup_model(config, device)
@@ -613,6 +804,8 @@ def run_training_experiment(config):
     resume_epoch = handle_checkpoint_resumption(
         config, model, optimizer, dataloader_generator, logger
     )
+
+    log_model_architecture(model, logger)
     
     # Initialize loss criterion
     if config['criterion'] == 'MSELoss':
@@ -644,22 +837,25 @@ def run_training_experiment(config):
     logger.info(f"\nNumber of trainable parameters: {count_trainable_parameters(model)}\n")
     
     # Run training
-    train_model(
-        model=model,
-        train_loader=train_loader,
-        test_loader=test_loader,
-        device=device,
-        optimizer=optimizer,
-        criterion=criterion,
-        epochs=config['epochs'],
-        training_results_save_path=training_results_save_path,
-        logger=logger,
-        early_stopping_patience=config['early_stopping_patience'],
-        save_path=save_path,
-        random_state_save_path=random_state_save_path,
-        dataloader_generator=dataloader_generator,
-        vision_layers=config['vision_layers'],
-        transformer_layers=config['transformer_layers'],
-        perturb_strategy=perturb_strategy,
-        start_epoch=resume_epoch,
-    )
\ No newline at end of file
+    try:
+        train_model(
+            model=model,
+            train_loader=train_loader,
+            test_loader=test_loader,
+            device=device,
+            optimizer=optimizer,
+            criterion=criterion,
+            epochs=config['epochs'],
+            training_results_save_path=training_results_save_path,
+            logger=logger,
+            early_stopping_patience=config['early_stopping_patience'],
+            save_path=save_path,
+            random_state_save_path=random_state_save_path,
+            dataloader_generator=dataloader_generator,
+            vision_layers=config['vision_layers'],
+            transformer_layers=config['transformer_layers'],
+            perturb_strategy=perturb_strategy,
+            start_epoch=resume_epoch,
+        )
+    finally:
+        wandb.finish()
\ No newline at end of file
