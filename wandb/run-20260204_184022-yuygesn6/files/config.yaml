_wandb:
    value:
        cli_version: 0.24.0
        code_path: code/scripts/run_training.py
        e:
            a8cbe5rv03dqy3fld3ves7ye0arvny2u:
                args:
                    - --config
                    - /tmp/tmp.zGApFa9feh/image_noise_epoch0_len5.yaml
                codePath: scripts/run_training.py
                codePathLocal: scripts/run_training.py
                cpu_count: 16
                cpu_count_logical: 32
                cudaVersion: "12.7"
                disk:
                    /:
                        total: "1888559353856"
                        used: "1780338638848"
                email: mej.jenkins@gmail.com
                executable: /home/wallacelab/miniconda/envs/adaptive-clip/bin/python3
                git:
                    commit: 50c0ca479db35b6ce46588b2f441425eeab7f63a
                    remote: https://github.com/marrenj/cvpr_perturbations.git
                gpu: NVIDIA RTX A6000
                gpu_count: 2
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-ac1e7360-8b1e-ed4b-f978-73fc25e7633b
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-b0d28ed1-ef1b-ccca-52e6-4d98c79c3624
                host: ASWALL1PD5CYD2D.vuds.vanderbilt.edu
                memory:
                    total: "270093541376"
                os: Linux-6.8.0-90-generic-x86_64-with-glibc2.35
                program: /home/wallacelab/Documents/GitHub/cvpr_perturbations/scripts/run_training.py
                python: CPython 3.11.13
                root: /home/wallacelab/Documents/GitHub/cvpr_perturbations
                startedAt: "2026-02-05T00:40:22.214314Z"
                writerId: a8cbe5rv03dqy3fld3ves7ye0arvny2u
        m: []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 53
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 53
            "3":
                - 1
                - 2
                - 13
                - 15
                - 16
                - 62
            "4": 3.11.13
            "5": 0.24.0
            "6": 4.57.0
            "12": 0.24.0
            "13": linux-x86_64
backbone:
    value: ViT-L/14
baseline_checkpoint_path:
    value: /home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test/baseline_seed1/vit_l_14_rank32_perturb-type-none_init-seed1behavioral-rsa-True
batch_size:
    value: 64
behavioral_rsa:
    value: true
criterion:
    value: MSELoss
cuda:
    value: 1
dataset_size:
    value: 1806
dataset_type:
    value: things
early_stopping_patience:
    value: 20
epochs:
    value: 500
experiment_type:
    value: baseline
img_annotations_file:
    value: ./data/spose_embedding66d_rescaled_1806train.csv
img_dir:
    value: /home/wallacelab/investigating-complexity/Images/THINGS
logger:
    value: None
lr:
    value: 0.0003
model_rdm_distance_metric:
    value: pearson
perturb_epoch:
    value: 0
perturb_length:
    value: 5
perturb_seed:
    value: 1
perturb_type:
    value: image_noise
random_seed:
    value: 1
rank:
    value: 32
rsa_annotations_file:
    value: ./data/spose_embedding66d_rescaled_1806train.csv
rsa_similarity_metric:
    value: spearman
save_path:
    value: /home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test/image_noise_perturb_seed1/epoch0_length5/vit_l_14_rank32_perturb-type-image_noise_epoch0_length5_perturb-seed1_init-seed1behavioral-rsa-True
test_size:
    value: 362
total_parameters:
    value: 427802369
train_portion:
    value: 0.8
train_size:
    value: 1444
trainable_layers:
    value:
        - clip_model.visual.transformer.resblocks.22.attn.out_proj.m
        - clip_model.visual.transformer.resblocks.22.attn.out_proj.delta_D_A
        - clip_model.visual.transformer.resblocks.22.attn.out_proj.delta_D_B
        - clip_model.visual.transformer.resblocks.23.attn.out_proj.m
        - clip_model.visual.transformer.resblocks.23.attn.out_proj.delta_D_A
        - clip_model.visual.transformer.resblocks.23.attn.out_proj.delta_D_B
        - clip_model.transformer.resblocks.11.attn.out_proj.m
        - clip_model.transformer.resblocks.11.attn.out_proj.delta_D_A
        - clip_model.transformer.resblocks.11.attn.out_proj.delta_D_B
trainable_parameters:
    value: 183040
trainable_percentage:
    value: 0.04278611182725826
transformer_layers:
    value: 1
vision_layers:
    value: 2
wandb_entity:
    value: null
wandb_project:
    value: temporal-dynamics-of-human-alignment
wandb_run_name:
    value: vit_l_14_rank32_perturb-type-image_noise_epoch0_length5_perturb-seed1_init-seed1behavioral-rsa-True
