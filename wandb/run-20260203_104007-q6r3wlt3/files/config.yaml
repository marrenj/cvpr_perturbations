_wandb:
    value:
        cli_version: 0.24.0
        code_path: code/scripts/run_training.py
        e:
            b615haj17y9ersmasodr209xgul2vqma:
                args:
                    - --config
                    - configs/training_config.yaml
                codePath: scripts/run_training.py
                codePathLocal: scripts/run_training.py
                cpu_count: 16
                cpu_count_logical: 32
                cudaVersion: "12.7"
                disk:
                    /:
                        total: "1888559353856"
                        used: "1766275067904"
                email: mej.jenkins@gmail.com
                executable: /home/wallacelab/miniconda/envs/adaptive-clip/bin/python
                git:
                    commit: c251ae09172f6a89e266317c79168f86ea61eb16
                    remote: https://github.com/marrenj/cvpr_perturbations.git
                gpu: NVIDIA RTX A6000
                gpu_count: 2
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-ac1e7360-8b1e-ed4b-f978-73fc25e7633b
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-b0d28ed1-ef1b-ccca-52e6-4d98c79c3624
                host: ASWALL1PD5CYD2D.vuds.vanderbilt.edu
                memory:
                    total: "270093541376"
                os: Linux-6.8.0-90-generic-x86_64-with-glibc2.35
                program: /home/wallacelab/Documents/GitHub/cvpr_perturbations/scripts/run_training.py
                python: CPython 3.11.13
                root: /home/wallacelab/Documents/GitHub/cvpr_perturbations
                startedAt: "2026-02-03T16:40:07.242197Z"
                writerId: b615haj17y9ersmasodr209xgul2vqma
        m: []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 53
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 53
            "3":
                - 1
                - 2
                - 13
                - 15
                - 16
                - 62
            "4": 3.11.13
            "5": 0.24.0
            "6": 4.57.0
            "12": 0.24.0
            "13": linux-x86_64
backbone:
    value: ViT-L/14
baseline_checkpoint_path:
    value: null
batch_size:
    value: 64
behavioral_rsa:
    value: true
criterion:
    value: MSELoss
cuda:
    value: 1
dataset_size:
    value: 1806
dataset_type:
    value: things
early_stopping_patience:
    value: 20
epochs:
    value: 500
experiment_type:
    value: baseline
img_annotations_file:
    value: ./data/spose_embedding66d_rescaled_1806train.csv
img_dir:
    value: /home/wallacelab/investigating-complexity/Images/THINGS
logger:
    value: None
lr:
    value: 0.0003
model_rdm_distance_metric:
    value: pearson
perturb_epoch:
    value: 0
perturb_length:
    value: 0
perturb_seed:
    value: null
perturb_type:
    value: none
random_seed:
    value: 4
rank:
    value: 32
rsa_annotations_file:
    value: ./data/spose_embedding66d_rescaled_1806train.csv
rsa_similarity_metric:
    value: spearman
save_path:
    value: /home/wallacelab/teba/multimodal_brain_inspired/marren/temporal_dynamics_of_human_alignment/test/baseline_seed4/vit_l_14_rank32_perturb-type-none_init-seed4behavioral-rsa-True
test_size:
    value: 362
total_parameters:
    value: 427802369
train_portion:
    value: 0.8
train_size:
    value: 1444
trainable_layers:
    value:
        - clip_model.visual.transformer.resblocks.22.attn.out_proj.m
        - clip_model.visual.transformer.resblocks.22.attn.out_proj.delta_D_A
        - clip_model.visual.transformer.resblocks.22.attn.out_proj.delta_D_B
        - clip_model.visual.transformer.resblocks.23.attn.out_proj.m
        - clip_model.visual.transformer.resblocks.23.attn.out_proj.delta_D_A
        - clip_model.visual.transformer.resblocks.23.attn.out_proj.delta_D_B
        - clip_model.transformer.resblocks.11.attn.out_proj.m
        - clip_model.transformer.resblocks.11.attn.out_proj.delta_D_A
        - clip_model.transformer.resblocks.11.attn.out_proj.delta_D_B
trainable_parameters:
    value: 183040
trainable_percentage:
    value: 0.04278611182725826
transformer_layers:
    value: 1
vision_layers:
    value: 2
wandb_entity:
    value: null
wandb_project:
    value: temporal-dynamics-of-human-alignment
wandb_run_name:
    value: vit_l_14_rank32_perturb-type-none_init-seed4behavioral-rsa-True
